---
title: List of Deep Learning Papers
sidebar:
  nav: docs-research
aside:
  toc: true
key: 20250710_list_DL
tags: 
lang: ko
---
# ğŸ“„ ëŒ€í‘œ ë…¼ë¬¸ë“¤

### **1980s ~ 2000s: íƒœë™ê¸°**

\[1980\] Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position â€“ Fukushima

\[1986\] Learning representations by back-propagating errors â€“ Rumelhart, Hinton, Williams

\[1988\] Gradient-Based Learning Applied to Document Recognition â€“ LeCun et al. (LeNet-5)


### <span style="color: brown">**2012 ~ 2014: ë”¥ëŸ¬ë‹ ëŒ€ì¤‘í™” (CNN ì¤‘ì‹¬)**</span>

\[2012\] ImageNet Classification with Deep Convolutional Neural Networks â€“ Krizhevsky, Sutskever, Hinton (AlexNet)

\[2014\] Very Deep Convolutional Networks for Large-Scale Image Recognition â€“ Simonyan, Zisserman (VGGNet)

\[2014\] Going Deeper with Convolutions â€“ Szegedy et al. (GoogLeNet/Inception)

\[2014\] Generative Adversarial Nets â€“ Goodfellow et al. (GAN)

\[2014\] Neural Machine Translation by Jointly Learning to Align and Translate â€“ Bahdanau et al. (Attention in Seq2Seq)


### <span style="color: brown">**2015 ~ 2017: êµ¬ì¡° í˜ì‹ ê³¼ Transformer ë“±ì¥**</span>

\[2015\] Deep Residual Learning for Image Recognition â€“ He et al. (ResNet)

\[2015\] U-Net: Convolutional Networks for Biomedical Image Segmentation â€“ Ronneberger et al.

\[2015\] Human-level control through deep reinforcement learning â€“ Mnih et al. (DQN)

\[2017\] Attention Is All You Need â€“ Vaswani et al. (Transformer)


### <span style="color: brown">**2018 ~ 2020: LLMê³¼ ì‚¬ì „í•™ìŠµ í˜ì‹ **</span>

\[2018\] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding â€“ Devlin et al.

\[2018\] Improving Language Understanding by Generative Pre-training â€“ Radford et al. (GPT-1)

\[2019\] Language Models are Unsupervised Multitask Learners â€“ Radford et al. (GPT-2, ë…¼ë¬¸í™” ì—†ì´ ë³´ê³ ì„œ í˜•ì‹)

\[2019\] RoBERTa: A Robustly Optimized BERT Pretraining Approach â€“ Liu et al.

\[2019] XLNet: Generalized Autoregressive Pretraining for Language Understanding â€“ Yang et al.

\[2020\] T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer â€“ Raffel et al.


### <span style="color: brown">**2021 ~ 2022: Vision Transformer, ë©€í‹°ëª¨ë‹¬, ìƒì„±ëª¨ë¸ ì§„í™”**</span>

\[2021\] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale â€“ Dosovitskiy et al. (ViT)

\[2021\] Learning Transferable Visual Models From Natural Language Supervision â€“ Radford et al. (CLIP)

\[2021\] Zero-Shot Text-to-Image Generation â€“ Ramesh et al. (DALLÂ·E)

\[2022\] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding â€“ Saharia et al. (Imagen, by Google)

\[2022\] Flamingo: A Visual Language Model for Few-Shot Learning â€“ DeepMind


### <span style="color: brown">**2023 ~ 2024: ë©€í‹°ëª¨ë‹¬ ëŒ€í†µí•© + AGI ì „ì´ˆ**</span>

\[2023\] GPT-4 Technical Report â€“ OpenAI

\[2023\] Segment Anything â€“ Kirillov et al. (Meta)

\[2023\] LLaMA: Open and Efficient Foundation Language Models â€“ Touvron et al. (Meta)

\[2023\] LLaVA: Visual Instruction Tuning â€“ Liu et al.

\[2024\] GPT-4o: Omni-modal AI by OpenAI â€“ OpenAI

\[2024\] Gemini 1.5 Technical Report â€“ Google DeepMind



