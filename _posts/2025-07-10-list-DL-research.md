---
title: List of Deep Learning Papers
sidebar:
  nav: docs-research
aside:
  toc: true
key: 20250710_list_DL
tags: 
lang: ko
---
# 📄 대표 논문들

### **1980s ~ 2000s: 태동기**

&emsp;\[1980\] Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position – Fukushima

&emsp;\[1986\] Learning representations by back-propagating errors – Rumelhart, Hinton, Williams

&emsp;\[1988\] Gradient-Based Learning Applied to Document Recognition – LeCun et al. (LeNet-5)


### <span style="color: brown">**2012 ~ 2014: 딥러닝 대중화 (CNN 중심)**</span>

&emsp;\[2012\] ImageNet Classification with Deep Convolutional Neural Networks – Krizhevsky, Sutskever, Hinton (AlexNet)

&emsp;\[2014\] Very Deep Convolutional Networks for Large-Scale Image Recognition – Simonyan, Zisserman (VGGNet)

&emsp;\[2014\] Going Deeper with Convolutions – Szegedy et al. (GoogLeNet/Inception)

&emsp;\[2014\] Generative Adversarial Nets – Goodfellow et al. (GAN)

&emsp;\[2014\] Neural Machine Translation by Jointly Learning to Align and Translate – Bahdanau et al. (Attention in Seq2Seq)


### <span style="color: brown">**2015 ~ 2017: 구조 혁신과 Transformer 등장**</span>

&emsp;\[2015\] Deep Residual Learning for Image Recognition – He et al. (ResNet)

&emsp;\[2015\] U-Net: Convolutional Networks for Biomedical Image Segmentation – Ronneberger et al.

&emsp;\[2015\] Human-level control through deep reinforcement learning – Mnih et al. (DQN)

&emsp;\[2017\] Attention Is All You Need – Vaswani et al. (Transformer)


### <span style="color: brown">**2018 ~ 2020: LLM과 사전학습 혁신**</span>

&emsp;\[2018\] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding – Devlin et al.

&emsp;\[2018\] Improving Language Understanding by Generative Pre-training – Radford et al. (GPT-1)

&emsp;\[2019\] Language Models are Unsupervised Multitask Learners – Radford et al. (GPT-2, 논문화 없이 보고서 형식)

&emsp;\[2019\] RoBERTa: A Robustly Optimized BERT Pretraining Approach – Liu et al.

&emsp;\[2019] XLNet: Generalized Autoregressive Pretraining for Language Understanding – Yang et al.

&emsp;\[2020\] T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer – Raffel et al.


### <span style="color: brown">**2021 ~ 2022: Vision Transformer, 멀티모달, 생성모델 진화**</span>

&emsp;\[2021\] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale – Dosovitskiy et al. (ViT)

&emsp;\[2021\] Learning Transferable Visual Models From Natural Language Supervision – Radford et al. (CLIP)

&emsp;\[2021\] Zero-Shot Text-to-Image Generation – Ramesh et al. (DALL·E)

&emsp;\[2022\] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding – Saharia et al. (Imagen, by Google)

&emsp;\[2022\] Flamingo: A Visual Language Model for Few-Shot Learning – DeepMind


### <span style="color: brown">**2023 ~ 2024: 멀티모달 대통합 + AGI 전초**</span>

&emsp;\[2023\] GPT-4 Technical Report – OpenAI

&emsp;\[2023\] Segment Anything – Kirillov et al. (Meta)

&emsp;\[2023\] LLaMA: Open and Efficient Foundation Language Models – Touvron et al. (Meta)

&emsp;\[2023\] LLaVA: Visual Instruction Tuning – Liu et al.

&emsp;\[2024\] GPT-4o: Omni-modal AI by OpenAI – OpenAI

&emsp;\[2024\] Gemini 1.5 Technical Report – Google DeepMind



